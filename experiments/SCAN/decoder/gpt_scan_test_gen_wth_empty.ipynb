{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975b8951-366d-4add-aa11-8291ec8a1bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdo/anaconda3/envs/transformer-test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54539932-d248-4c9f-9085-ec4b3a633196",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/drdo/Caricatures/models/scan_dummy_tokens_gpt2/checkpoint-40000\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"/home/drdo/Caricatures/models/scan_dummy_tokens_gpt2/checkpoint-40000\")\n",
    "\n",
    "model.generation_config.max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c75252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 9\n",
    "dummy_token = \"<empty>\"\n",
    "\n",
    "# command type maps\n",
    "actions = {\n",
    "    \"walk\": \"I_WALK\",\n",
    "    \"run\": \"I_RUN\",\n",
    "    \"jump\": \"I_JUMP\",\n",
    "    \"look\": \"I_LOOK\",\n",
    "    \"turn\": dummy_token,\n",
    "    dummy_token: dummy_token,\n",
    "    }\n",
    "\n",
    "turns = {\n",
    "    \"around\": \"yyyy\",\n",
    "    \"opposite\": \"yy\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "directions = {\n",
    "    \"right\": \"I_TURN_RIGHT\",\n",
    "    \"left\": \"I_TURN_LEFT\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "nums = {\n",
    "    \"twice\": \"xx\",\n",
    "    \"thrice\": \"xxx\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "conjs = [\"and\", \"after\", dummy_token]\n",
    "\n",
    "# command structure\n",
    "command_structure = {\n",
    "    0: actions,\n",
    "    1: turns,\n",
    "    2: directions,\n",
    "    3: nums,\n",
    "    4: conjs,\n",
    "    5: actions,\n",
    "    6: turns,\n",
    "    7: directions,\n",
    "    8: nums,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8598b0-3238-48c4-84e2-4a10926400fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"scan\", \"simple\", trust_remote_code=True)\n",
    "column_names = dataset[\"train\"].column_names\n",
    "input_column = column_names[0]\n",
    "output_column = column_names[1]\n",
    "\n",
    "def add_empty_token(x):\n",
    "    command_str = x[input_column]\n",
    "    command = command_str.split()\n",
    "    padded_command = []\n",
    "    index = 0\n",
    "    c = 0\n",
    "    while index < max_len:\n",
    "        expected_cs = command_structure[index]\n",
    "        if c < len(command) and command[c] in expected_cs:\n",
    "            padded_command.append(command[c])\n",
    "            c += 1\n",
    "        else:\n",
    "            padded_command.append(dummy_token)\n",
    "        index += 1\n",
    "    \n",
    "    x[input_column] = ' '.join(padded_command)\n",
    "    return x\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    add_empty_token,\n",
    "    batched=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a0ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'commands': 'run <empty> right twice after walk <empty> right twice',\n",
       " 'actions': 'I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8c4ba5-c140-443e-9aa9-38d98ba5b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_inputs from input_ids : torch.Size([1, 12])\n",
      "None\n",
      "output logits : torch.Size([1, 12, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 13])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 14])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 15])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 16])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 17])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 18])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 19])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 20])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 21])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 22])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 23])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 24])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 25])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 26])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 27])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 28])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 29])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 30])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 31])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 32])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 33])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 34])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 35])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 36])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 37])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 38])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 39])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 40])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 41])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 42])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 43])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 44])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 45])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 46])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 47])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 48])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 49])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 50])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 51])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 52])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 53])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 54])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 55])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 56])\n",
      "\n",
      "model_inputs from input_ids : torch.Size([1, 1])\n",
      "past_key_values : 12\n",
      "output logits : torch.Size([1, 1, 50260])\n",
      "torch.Size([1, 50260])\n",
      "new input_ids : torch.Size([1, 57])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generation_mode = GenerationMode.GREEDY_SEARCH\n",
    "\n",
    "context = 'run <empty> right twice after walk <empty> right twice'\n",
    "inputs = tokenizer(context+tokenizer.sep_token, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(**inputs)[0]\n",
    "#print(output)\n",
    "#print(tokenizer.decode(output, skip_special_tokens=False))\n",
    "\n",
    "#tokenizer.decode(output[0], skip_special_tokens=False).replace(context+tokenizer.sep_token, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b2414ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run <empty> right twice after walk <empty> right twice\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([ 5143, 220, 50259, 826, 5403, 706, 2513, 220, 50259, 826, 5403])\n",
    "print(tokenizer.decode(a, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0955fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49eb2758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'run <empty> right twice after walk <empty> right twice<sep>I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN<|endoftext|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 'run <empty> right twice after walk <empty> right twice'\n",
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "tokenizer.decode(model.generate(**inputs)[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db52e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = test_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d77f4f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e19a6d27f164429be9eb6dab1f6dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "model.to(\"cuda\")\n",
    "bar = tqdm(range(len(testset)))\n",
    "for example in testset:\n",
    "    command = example['commands']\n",
    "    label = example['actions']\n",
    "    inputs = tokenizer(command+tokenizer.sep_token, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs)[0].to(\"cpu\")\n",
    "    output = tokenizer.decode(output, skip_special_tokens=False).replace(command+tokenizer.sep_token, '')\n",
    "    output = output.replace(tokenizer.eos_token, '')\n",
    "    if output == label:\n",
    "        count += 1\n",
    "    bar.update(1)\n",
    "\n",
    "print(count/len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilgpt2 : acc = 0.6406025824964132\n",
    "# distilgpt2 : acc = 0.67 -> beam search (beam size = 3)\n",
    "# gpt2 : acc = 0.810856049736968, 0.88, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
