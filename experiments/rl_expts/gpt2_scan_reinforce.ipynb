{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets accelerate evaluate tensorboard nnsight wandb\n",
    "#!wandb login\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, default_data_collator,\n",
    "    get_scheduler, AutoModelForCausalLM,\n",
    "    AutoConfig, GenerationConfig,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal model\n",
    "# longest command is 9 words : https://arxiv.org/pdf/1711.00350\n",
    "max_len = 9\n",
    "dummy_token = \"<empty>\"\n",
    "\n",
    "# command type maps\n",
    "actions = {\n",
    "    \"walk\": \"I_WALK\",\n",
    "    \"run\": \"I_RUN\",\n",
    "    \"jump\": \"I_JUMP\",\n",
    "    \"look\": \"I_LOOK\",\n",
    "    \"turn\": dummy_token,\n",
    "    dummy_token: dummy_token,\n",
    "    }\n",
    "\n",
    "turns = {\n",
    "    \"around\": \"yyyy\",\n",
    "    \"opposite\": \"yy\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "directions = {\n",
    "    \"right\": \"I_TURN_RIGHT\",\n",
    "    \"left\": \"I_TURN_LEFT\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "nums = {\n",
    "    \"twice\": \"xx\",\n",
    "    \"thrice\": \"xxx\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "conjs = [\"and\", \"after\", dummy_token]\n",
    "\n",
    "# command structure\n",
    "command_structure = {\n",
    "    0: actions,\n",
    "    1: turns,\n",
    "    2: directions,\n",
    "    3: nums,\n",
    "    4: conjs,\n",
    "    5: actions,\n",
    "    6: turns,\n",
    "    7: directions,\n",
    "    8: nums,\n",
    "}\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "\n",
    "# model, tokenizer\n",
    "# need to load a 'half-trained' model\n",
    "#model_name_or_path = 'distilbert/distilgpt2'\n",
    "#model_name_or_path = '/root/Caricatures/models/distilgpt2_40k'\n",
    "#model_name_or_path = '/users/ujan/caricatures/models/scan/distilgpt2_40k'\n",
    "#model_name_or_path = '/home/drdo/Caricatures/models/scan_distilgpt2/checkpoint-40000'\n",
    "model_name_or_path = '/home/drdo/Caricatures/models/scan_distilgpt2/checkpoint-5000'\n",
    "special_tokens_dict = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"sep_token\": \"<sep>\",\n",
    "}\n",
    "\n",
    "# dataset\n",
    "dataset = \"scan\"\n",
    "# 'simple', 'addprim_jump', 'addprim_turn_left', 'filler_num0', \n",
    "# 'filler_num1', 'filler_num2', 'filler_num3', 'length', \n",
    "# 'template_around_right', 'template_jump_around_right', \n",
    "# 'template_opposite_right', 'template_right'\n",
    "dataset_config = \"simple\"\n",
    "validation_split = 0.1\n",
    "max_source_length = 512\n",
    "max_target_length = 512\n",
    "max_gen_length = 256\n",
    "\n",
    "# training\n",
    "#output_dir = '/users/ujan/caricatures/models/scan/scan_distilgpt2_reinforce'\n",
    "output_dir = '/home/drdo/Caricatures/models/distilgpt2_reinforce'\n",
    "#output_dir = '/root/Caricatures/models/distilgpt2_reinforce'\n",
    "num_workers = os.cpu_count()  # 1, None, 32\n",
    "per_device_train_batch_size = 64# 64\n",
    "per_device_eval_batch_size = 64  # 64\n",
    "train_steps = 100000\n",
    "warmup_steps = 0\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 100\n",
    "lr = 5e-8\n",
    "weight_decay = 0.0\n",
    "lr_scheduler_type = 'linear'\n",
    "mixed_precision = 'no'\n",
    "num_beams = 1\n",
    "report_to = 'wandb'\n",
    "EPSILON = 1e-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "accelerator_log_kwargs = {}\n",
    "accelerator_log_kwargs[\"log_with\"] = report_to\n",
    "accelerator_log_kwargs[\"project_dir\"] = output_dir\n",
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps, **accelerator_log_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset, dataset_config, trust_remote_code=True)\n",
    "\n",
    "# split train set into train and validation\n",
    "train_val_split = raw_datasets['train'].train_test_split(test_size=validation_split, seed=seed)\n",
    "raw_datasets['train'] = train_val_split['train']\n",
    "raw_datasets['validation'] = train_val_split['test']\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "input_column = column_names[0]\n",
    "output_column = column_names[1]\n",
    "\n",
    "# format dataset with dummy tokens\n",
    "special_tokens_dict[\"additional_special_tokens\"] = [dummy_token]\n",
    "\n",
    "def add_empty_token(x):\n",
    "    command_str = x[input_column]\n",
    "    command = command_str.split()\n",
    "    padded_command = []\n",
    "    index = 0\n",
    "    c = 0\n",
    "    while index < max_len:\n",
    "        expected_cs = command_structure[index]\n",
    "        if c < len(command) and command[c] in expected_cs:\n",
    "            padded_command.append(command[c])\n",
    "            c += 1\n",
    "        else:\n",
    "            padded_command.append(dummy_token)\n",
    "        index += 1\n",
    "    \n",
    "    x[input_column] = ' '.join(padded_command)\n",
    "    return x\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    raw_datasets[\"train\"] = raw_datasets[\"train\"].map(\n",
    "        add_empty_token,\n",
    "        batched=False,\n",
    "        num_proc=num_workers, \n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    raw_datasets[\"validation\"] = raw_datasets[\"validation\"].map(\n",
    "        add_empty_token,\n",
    "        batched=False,\n",
    "        num_proc=num_workers,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# left padding for batch generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "# Resize the embeddings only when necessary to avoid index errors\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Generation config\n",
    "generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n",
    "#gen_dict = generation_config.to_dict()\n",
    "#gen_dict[\"language\"] = model_lang\n",
    "# reload with new attributes\n",
    "#generation_config = GenerationConfig.from_dict(gen_dict)\n",
    "#max_gen_length = model.config.max_length\n",
    "#num_beams = args.num_beams if args.num_beams is not None else model.config.num_beams\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "gen_kwargs = {\"max_new_tokens\": max_gen_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset\n",
    "def preprocess_function(examples):\n",
    "    # commands, actions\n",
    "    inputs = examples[input_column]\n",
    "    targets = examples[output_column]\n",
    "\n",
    "    # tokenize as single sequence separated by special token\n",
    "    model_inputs = tokenizer(\n",
    "        [i+tokenizer.sep_token for i in inputs],\n",
    "        padding='max_length', max_length=max_source_length\n",
    "    )\n",
    "    # labels same as inputs. labels shifted right in the model forward by default\n",
    "    model_inputs['labels'] = tokenizer(\n",
    "        [t+tokenizer.eos_token for t in targets],\n",
    "        padding='max_length', max_length=max_source_length\n",
    "    )['input_ids']\n",
    "    # set label padding to -100 \n",
    "    #model_inputs['labels'] = [\n",
    "        #[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_inputs['labels']\n",
    "    #]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    train_dataset = raw_datasets[\"train\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    eval_dataset = raw_datasets[\"validation\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator and loaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=train_steps * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare everything for accelerator\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45191379bc3d4752a1b629754da66b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749074074074074\n"
     ]
    }
   ],
   "source": [
    "eval_bar = tqdm(range(len(eval_dataloader)), position=0)\n",
    "accuracy = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    with torch.no_grad():\n",
    "        output_ids = accelerator.unwrap_model(model).generate(\n",
    "            **batch,\n",
    "            generation_config=generation_config,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "    # pad_acrss_processes to get equal length for each processs\n",
    "    output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "    label_ids = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "    # gather\n",
    "    output_ids = accelerator.gather(output_ids) \n",
    "    label_ids = accelerator.gather(label_ids)  \n",
    "    # decode\n",
    "    batch_output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    batch_input = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n",
    "    outputs = [batch_output[b].replace(batch_input[b], '') for b in range(len(batch_output))]\n",
    "    labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    # compute accuracy\n",
    "    acc = [o==l for o, l in zip(outputs, labels)]\n",
    "    accuracy += sum(acc)/len(acc)\n",
    "\n",
    "    eval_bar.update(1)\n",
    "\n",
    "print(accuracy/len(eval_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right padding for logits\n",
    "tokenizer.padding_side = \"right\"\n",
    "ignore_index = -100\n",
    "\n",
    "\n",
    "# re-tokenize left padded sequences need for batch generation to right padded sequences\n",
    "def re_tokenize(token_ids):\n",
    "    tokens = tokenizer.batch_decode(token_ids, skip_special_tokens=False)\n",
    "    tokens = [o.replace(tokenizer.pad_token, '') for o in tokens]\n",
    "    tokens = [o.replace(tokenizer.eos_token, '') for o in tokens]\n",
    "    tokenized_tokens = tokenizer(\n",
    "        tokens,\n",
    "        padding='max_length',\n",
    "        max_length=max_source_length,\n",
    "        return_tensors='pt',\n",
    "    ).to(model.device)\n",
    "    input_ids = tokenized_tokens['input_ids']\n",
    "    attention_mask = tokenized_tokens['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def prepare_input_for_rl_step(output_ids, gen_label_ids):\n",
    "\n",
    "    generated_ids, attention_mask = re_tokenize(output_ids)\n",
    "    gen_label_ids, _ = re_tokenize(gen_label_ids) \n",
    "    # context labels needed for ce loss for context\n",
    "    # get only context labels\n",
    "    all_tokens = tokenizer.batch_decode(generated_ids)\n",
    "    context_tokens = [t.split(tokenizer.sep_token)[0] for t in all_tokens]\n",
    "    tokenized_context = tokenizer(\n",
    "        [c+tokenizer.sep_token for c in context_tokens],\n",
    "        padding='max_length',\n",
    "        max_length=max_source_length,\n",
    "        return_tensors='pt',\n",
    "    ).to(model.device)\n",
    "    context_label_ids = tokenized_context['input_ids']\n",
    "    # set context label padding to -100 \n",
    "    context_label_ids = [\n",
    "        [\n",
    "            (l if l != tokenizer.pad_token_id else ignore_index) for l in label\n",
    "        ] for label in context_label_ids.tolist()\n",
    "    ]\n",
    "    context_label_ids = torch.tensor(context_label_ids).to(model.device)\n",
    "\n",
    "    return generated_ids, attention_mask, gen_label_ids, context_label_ids\n",
    "    \n",
    "\n",
    "# returns 1 if accurate, 0 otherwise\n",
    "# experiment with other rewards\n",
    "def binary_reward_function(output_ids, gen_label_ids):\n",
    "    # decode output\n",
    "    output_tokens = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    output_tokens = [\n",
    "        o.replace(tokenizer.pad_token, '').split(tokenizer.sep_token)[1] for o in output_tokens\n",
    "    ]\n",
    "    # decode labels\n",
    "    label_tokens = tokenizer.batch_decode(gen_label_ids, skip_special_tokens=False)\n",
    "    label_tokens = [l.replace(tokenizer.pad_token, '') for l in label_tokens]\n",
    "    # compute reward(=accuracy)\n",
    "    reward = [o==l for o, l in zip(output_tokens, label_tokens)]\n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(model.device)\n",
    "    return reward\n",
    "\n",
    "\n",
    "def rouge1_reward_function(output_ids, gen_label_ids):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'])\n",
    "    # decode output\n",
    "    output_tokens = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    output_tokens = [\n",
    "        o.replace(tokenizer.pad_token, '').split(tokenizer.sep_token)[1] for o in output_tokens\n",
    "    ]\n",
    "    # decode labels\n",
    "    label_tokens = tokenizer.batch_decode(gen_label_ids, skip_special_tokens=False)\n",
    "    label_tokens = [l.replace(tokenizer.pad_token, '') for l in label_tokens]\n",
    "    # compute reward(=rouge1_f)\n",
    "    reward = [scorer.score(o,l)['rouge1'].fmeasure for o, l in zip(output_tokens, label_tokens)]\n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(model.device)\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "def logprobs_from_logits(logits, labels):\n",
    "    # https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591\n",
    "    logp = torch.log(F.softmax(logits, dim=2) + EPSILON)\n",
    "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logpy\n",
    "    \n",
    "\n",
    "def loss_function(logits, context_label_ids, gen_label_ids, attention_mask, reward):\n",
    "\n",
    "    # ce loss for context\n",
    "    # shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_context_labels = context_label_ids[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    context_loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), shift_context_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # reinforce loss\n",
    "    # logprobs -> b x seq_len\n",
    "    logprob = logprobs_from_logits(logits, gen_label_ids)\n",
    "    # zero out context positions in logits\n",
    "    logprob[context_label_ids != ignore_index] = 0\n",
    "    # zero out padding positions in logits\n",
    "    logprob[attention_mask == 0] = 0\n",
    "    # reshape reward\n",
    "    reward = reward.unsqueeze(1).repeat(1,logprob.shape[1])\n",
    "    reinforce_loss = -logprob * reward\n",
    "\n",
    "    # total loss\n",
    "    # zero out context from attention_mask\n",
    "    attention_mask[context_label_ids != ignore_index] = 0\n",
    "    reinforce_loss = torch.sum(reinforce_loss) / torch.sum(attention_mask)\n",
    "    total_loss = context_loss + reinforce_loss\n",
    "\n",
    "    return total_loss\n",
    "    \n",
    "    \n",
    "def reinforce_step(generated_ids, attention_mask, gen_label_ids, context_label_ids):\n",
    "    # calculate reward 'to go' (reinforce)\n",
    "    # reward -> batch_size\n",
    "    #reward = binary_reward_function(generated_ids, gen_label_ids)\n",
    "    reward = rouge1_reward_function(generated_ids, gen_label_ids)\n",
    "    # model forward\n",
    "    logits = model(input_ids=generated_ids, attention_mask=attention_mask).logits\n",
    "    # compute loss\n",
    "    loss = loss_function(logits, context_label_ids, gen_label_ids, attention_mask, reward)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e102fa19654140b7cfed926cf903bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea1f0a6ae014abb81052bc33fe80f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 23.61 GiB of which 364.44 MiB is free. Process 1279205 has 677.57 MiB memory in use. Including non-PyTorch memory, this process has 20.95 GiB memory in use. Of the allocated memory 20.41 GiB is allocated by PyTorch, and 93.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m generated_ids, attention_mask, gen_label_ids, context_label_ids \u001b[38;5;241m=\u001b[39m prepare_input_for_rl_step(output_ids, label_ids)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# reinforce\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mreinforce_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_label_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_label_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n\u001b[1;32m     36\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "Cell \u001b[0;32mIn[10], line 126\u001b[0m, in \u001b[0;36mreinforce_step\u001b[0;34m(generated_ids, attention_mask, gen_label_ids, context_label_ids)\u001b[0m\n\u001b[1;32m    124\u001b[0m reward \u001b[38;5;241m=\u001b[39m rouge1_reward_function(generated_ids, gen_label_ids)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# model forward\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(logits, context_label_ids, gen_label_ids, attention_mask, reward)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1119\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         output_attentions,\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:617\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    615\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    616\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 617\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    626\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:347\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    345\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    350\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:202\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights:\n\u001b[0;32m--> 202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattn_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Layer-wise attention scaling\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_by_inverse_layer_idx:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 23.61 GiB of which 364.44 MiB is free. Process 1279205 has 677.57 MiB memory in use. Including non-PyTorch memory, this process has 20.95 GiB memory in use. Of the allocated memory 20.41 GiB is allocated by PyTorch, and 93.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "global_step = 0  # tracks total steps\n",
    "\n",
    "progress_bar = tqdm(range(global_step, train_steps), disable=not accelerator.is_main_process, position=0)\n",
    "# eval bar\n",
    "eval_bar = tqdm(range(len(eval_dataloader)), position=1)\n",
    "\n",
    "while True:\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        with torch.no_grad():\n",
    "            output_ids = accelerator.unwrap_model(model).generate(\n",
    "                **batch,\n",
    "                generation_config=generation_config,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "\n",
    "        # pad_acrss_processes to get equal length for each processs\n",
    "        output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "        label_ids = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        # gather\n",
    "        output_ids = accelerator.gather(output_ids) \n",
    "        label_ids = accelerator.gather(label_ids)  \n",
    "\n",
    "        # re-tokenize for rl step\n",
    "        # generated_ids -> context ids + generated action ids\n",
    "        # attention mask -> attention mask for generated_ids\n",
    "        # gen_label_ids -> generated action ids\n",
    "        # context_label_ids -> context ids, needed to compute ce loss for context\n",
    "        generated_ids, attention_mask, gen_label_ids, context_label_ids = prepare_input_for_rl_step(output_ids, label_ids)\n",
    "\n",
    "        # reinforce\n",
    "        loss = reinforce_step(generated_ids, attention_mask, gen_label_ids, context_label_ids)\n",
    "\n",
    "        # backprop\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        if (global_step + 1) % eval_steps == 0:\n",
    "            model.eval()\n",
    "            accuracy = 0\n",
    "            \n",
    "            for batch in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    output_ids = accelerator.unwrap_model(model).generate(\n",
    "                        **batch,\n",
    "                        generation_config=generation_config,\n",
    "                        **gen_kwargs\n",
    "                    )\n",
    "\n",
    "                # pad_acrss_processes to get equal length for each processs\n",
    "                output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "                label_ids = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "                # gather\n",
    "                output_ids = accelerator.gather(output_ids) \n",
    "                label_ids = accelerator.gather(label_ids)  \n",
    "                # decode\n",
    "                batch_output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "                batch_input = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n",
    "                outputs = [batch_output[b].replace(batch_input[b], '') for b in range(len(batch_output))]\n",
    "                #accelerator.print(outputs)\n",
    "                labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "                # compute accuracy\n",
    "                acc = [o==l for o, l in zip(outputs, labels)]\n",
    "                accuracy += sum(acc)/len(acc)\n",
    "\n",
    "                eval_bar.update(1)\n",
    "\n",
    "            eval_bar.refresh()\n",
    "            eval_bar.reset()\n",
    "\n",
    "            accelerator.print('step : {}, accuracy  : {}'.format(global_step + 1, accuracy/len(eval_dataloader)))\n",
    "            accelerator.log({\n",
    "                \"accuracy\": accuracy/len(eval_dataloader),\n",
    "            },\n",
    "                step=global_step + 1,\n",
    "            )\n",
    "            \n",
    "        model.train()\n",
    "        global_step += 1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
