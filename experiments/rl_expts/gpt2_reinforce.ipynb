{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, default_data_collator,\n",
    "    get_scheduler, AutoModelForCausalLM,\n",
    "    AutoConfig, GenerationConfig,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal model\n",
    "# longest command is 9 words : https://arxiv.org/pdf/1711.00350\n",
    "max_len = 9\n",
    "dummy_token = \"<empty>\"\n",
    "\n",
    "# command type maps\n",
    "actions = {\n",
    "    \"walk\": \"I_WALK\",\n",
    "    \"run\": \"I_RUN\",\n",
    "    \"jump\": \"I_JUMP\",\n",
    "    \"look\": \"I_LOOK\",\n",
    "    \"turn\": dummy_token,\n",
    "    dummy_token: dummy_token,\n",
    "    }\n",
    "\n",
    "turns = {\n",
    "    \"around\": \"yyyy\",\n",
    "    \"opposite\": \"yy\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "directions = {\n",
    "    \"right\": \"I_TURN_RIGHT\",\n",
    "    \"left\": \"I_TURN_LEFT\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "nums = {\n",
    "    \"twice\": \"xx\",\n",
    "    \"thrice\": \"xxx\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "conjs = [\"and\", \"after\", dummy_token]\n",
    "\n",
    "# command structure\n",
    "command_structure = {\n",
    "    0: actions,\n",
    "    1: turns,\n",
    "    2: directions,\n",
    "    3: nums,\n",
    "    4: conjs,\n",
    "    5: actions,\n",
    "    6: turns,\n",
    "    7: directions,\n",
    "    8: nums,\n",
    "}\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "\n",
    "# model, tokenizer\n",
    "model_name_or_path = 'distilbert/distilgpt2'\n",
    "special_tokens_dict = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"sep_token\": \"<sep>\",\n",
    "}\n",
    "\n",
    "# dataset\n",
    "dataset = \"scan\"\n",
    "# 'simple', 'addprim_jump', 'addprim_turn_left', 'filler_num0', \n",
    "# 'filler_num1', 'filler_num2', 'filler_num3', 'length', \n",
    "# 'template_around_right', 'template_jump_around_right', \n",
    "# 'template_opposite_right', 'template_right'\n",
    "dataset_config = \"simple\"\n",
    "validation_split = 0.1\n",
    "max_source_length = 512\n",
    "max_target_length = 512\n",
    "max_gen_length = 256\n",
    "\n",
    "# training\n",
    "output_dir = '/root/Caricatures/models/scan_distilgpt2_reinforce'\n",
    "num_workers = os.cpu_count()  # 1, None, 32\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "train_steps = 100000\n",
    "warmup_steps = 0\n",
    "gradient_accumulation_steps = 1\n",
    "eval_steps = 5000\n",
    "lr = 5e-5\n",
    "weight_decay = 0.0\n",
    "lr_scheduler_type = 'linear'\n",
    "mixed_precision = 'no'\n",
    "num_beams = 1\n",
    "report_to = 'wandb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "accelerator_log_kwargs = {}\n",
    "accelerator_log_kwargs[\"log_with\"] = report_to\n",
    "accelerator_log_kwargs[\"project_dir\"] = output_dir\n",
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps, **accelerator_log_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset, dataset_config, trust_remote_code=True)\n",
    "\n",
    "# split train set into train and validation\n",
    "train_val_split = raw_datasets['train'].train_test_split(test_size=validation_split, seed=seed)\n",
    "raw_datasets['train'] = train_val_split['train']\n",
    "raw_datasets['validation'] = train_val_split['test']\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "input_column = column_names[0]\n",
    "output_column = column_names[1]\n",
    "\n",
    "# format dataset with dummy tokens\n",
    "special_tokens_dict[\"additional_special_tokens\"] = [dummy_token]\n",
    "\n",
    "def add_empty_token(x):\n",
    "    command_str = x[input_column]\n",
    "    command = command_str.split()\n",
    "    padded_command = []\n",
    "    index = 0\n",
    "    c = 0\n",
    "    while index < max_len:\n",
    "        expected_cs = command_structure[index]\n",
    "        if c < len(command) and command[c] in expected_cs:\n",
    "            padded_command.append(command[c])\n",
    "            c += 1\n",
    "        else:\n",
    "            padded_command.append(dummy_token)\n",
    "        index += 1\n",
    "    \n",
    "    x[input_column] = ' '.join(padded_command)\n",
    "    return x\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    raw_datasets[\"train\"] = raw_datasets[\"train\"].map(\n",
    "        add_empty_token,\n",
    "        batched=False,\n",
    "        num_proc=num_workers, \n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    raw_datasets[\"validation\"] = raw_datasets[\"validation\"].map(\n",
    "        add_empty_token,\n",
    "        batched=False,\n",
    "        num_proc=num_workers,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdo/anaconda3/envs/nlp/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/drdo/anaconda3/envs/nlp/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "# Resize the embeddings only when necessary to avoid index errors\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Generation config\n",
    "generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n",
    "#gen_dict = generation_config.to_dict()\n",
    "#gen_dict[\"language\"] = model_lang\n",
    "# reload with new attributes\n",
    "#generation_config = GenerationConfig.from_dict(gen_dict)\n",
    "#max_gen_length = model.config.max_length\n",
    "#num_beams = args.num_beams if args.num_beams is not None else model.config.num_beams\n",
    "gen_kwargs = {\"max_new_tokens\": max_gen_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3272303e6e4a90b67457752a009ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/15055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97afa1c948874479ba76455d07eedea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/1673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "def preprocess_function(examples):\n",
    "    # commands, actions\n",
    "    inputs = examples[input_column]\n",
    "    targets = examples[output_column]\n",
    "\n",
    "    # tokenize as single sequence separated by special token\n",
    "    model_inputs = tokenizer(\n",
    "        [i+tokenizer.sep_token for i in inputs],\n",
    "        [t+tokenizer.eos_token for t in targets],\n",
    "        padding='max_length', max_length=max_source_length\n",
    "    )\n",
    "    # labels same as inputs. labels shifted right in the model forward by default\n",
    "    model_inputs['labels'] = model_inputs['input_ids'].copy()\n",
    "    # set label padding to -100 \n",
    "    model_inputs['labels'] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_inputs['labels']\n",
    "    ]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    train_dataset = raw_datasets[\"train\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    eval_dataset = raw_datasets[\"validation\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator and loaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=train_steps * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare everything for accelerator\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436ffd5a64284e7bacfac85a2e1f362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a364176ad1ae41fcbffe8aacd667b99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768])\n",
      "torch.Size([16, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "global_step = 0  # tracks total steps\n",
    "\n",
    "progress_bar = tqdm(range(global_step, train_steps), disable=not accelerator.is_main_process, position=0)\n",
    "# eval bar\n",
    "eval_bar = tqdm(range(len(eval_dataloader)), position=1)\n",
    "\n",
    "while True:\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        output_ids = accelerator.unwrap_model(model).generate(\n",
    "            **batch,\n",
    "            generation_config=generation_config,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "        # pad_acrss_processes to get equal length for each processs\n",
    "        output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "        label_ids = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        output_ids = accelerator.gather(output_ids)  #.cpu().numpy()  # gather_for_metrics\n",
    "        label_ids = accelerator.gather(label_ids)  #.cpu().numpy()  # gather_for_metrics\n",
    "\n",
    "        print(output_ids.shape)\n",
    "        print(label_ids.shape)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
