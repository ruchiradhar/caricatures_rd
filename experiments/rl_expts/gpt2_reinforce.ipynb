{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets accelerate evaluate tensorboard nnsight wandb\n",
    "#!wandb login\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 20:51:25.258160: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, default_data_collator,\n",
    "    get_scheduler, AutoModelForCausalLM,\n",
    "    AutoConfig, GenerationConfig,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal model\n",
    "# longest command is 9 words : https://arxiv.org/pdf/1711.00350\n",
    "max_len = 9\n",
    "dummy_token = \"<empty>\"\n",
    "\n",
    "# command type maps\n",
    "actions = {\n",
    "    \"walk\": \"I_WALK\",\n",
    "    \"run\": \"I_RUN\",\n",
    "    \"jump\": \"I_JUMP\",\n",
    "    \"look\": \"I_LOOK\",\n",
    "    \"turn\": dummy_token,\n",
    "    dummy_token: dummy_token,\n",
    "    }\n",
    "\n",
    "turns = {\n",
    "    \"around\": \"yyyy\",\n",
    "    \"opposite\": \"yy\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "directions = {\n",
    "    \"right\": \"I_TURN_RIGHT\",\n",
    "    \"left\": \"I_TURN_LEFT\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "nums = {\n",
    "    \"twice\": \"xx\",\n",
    "    \"thrice\": \"xxx\",\n",
    "    dummy_token: dummy_token\n",
    "}\n",
    "\n",
    "conjs = [\"and\", \"after\", dummy_token]\n",
    "\n",
    "# command structure\n",
    "command_structure = {\n",
    "    0: actions,\n",
    "    1: turns,\n",
    "    2: directions,\n",
    "    3: nums,\n",
    "    4: conjs,\n",
    "    5: actions,\n",
    "    6: turns,\n",
    "    7: directions,\n",
    "    8: nums,\n",
    "}\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "\n",
    "# model, tokenizer\n",
    "# need to load a 'half-trained' model\n",
    "#model_name_or_path = 'distilbert/distilgpt2'\n",
    "#model_name_or_path = '/root/caricatures/models/distilgpt2_40k'\n",
    "model_name_or_path = '/users/ujan/caricatures/models/scan/distilgpt2_40k'\n",
    "#model_name_or_path = '/home/drdo/Caricatures/models/scan_distilgpt2/checkpoint-40000'\n",
    "special_tokens_dict = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"sep_token\": \"<sep>\",\n",
    "}\n",
    "\n",
    "# dataset\n",
    "dataset = \"scan\"\n",
    "# 'simple', 'addprim_jump', 'addprim_turn_left', 'filler_num0', \n",
    "# 'filler_num1', 'filler_num2', 'filler_num3', 'length', \n",
    "# 'template_around_right', 'template_jump_around_right', \n",
    "# 'template_opposite_right', 'template_right'\n",
    "dataset_config = \"simple\"\n",
    "validation_split = 0.1\n",
    "max_source_length = 512\n",
    "max_target_length = 512\n",
    "max_gen_length = 256\n",
    "\n",
    "# training\n",
    "output_dir = '/users/ujan/caricatures/models/scan/scan_distilgpt2_reinforce'\n",
    "#output_dir = '/home/drdo/Caricatures/models/distilgpt2_reinforce'\n",
    "num_workers = os.cpu_count()  # 1, None, 32/users/ujan/caricatures/models/scan/scan_distilgpt2_reinforce\n",
    "per_device_train_batch_size = 4 # 64\n",
    "per_device_eval_batch_size = 4  # 64\n",
    "train_steps = 100000\n",
    "warmup_steps = 0\n",
    "gradient_accumulation_steps = 1\n",
    "eval_steps = 5000\n",
    "lr = 5e-5\n",
    "weight_decay = 0.0\n",
    "lr_scheduler_type = 'linear'\n",
    "mixed_precision = 'no'\n",
    "num_beams = 1\n",
    "report_to = 'wandb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "accelerator_log_kwargs = {}\n",
    "accelerator_log_kwargs[\"log_with\"] = report_to\n",
    "accelerator_log_kwargs[\"project_dir\"] = output_dir\n",
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps, **accelerator_log_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset, dataset_config, trust_remote_code=True)\n",
    "\n",
    "# split train set into train and validation\n",
    "train_val_split = raw_datasets['train'].train_test_split(test_size=validation_split, seed=seed)\n",
    "raw_datasets['train'] = train_val_split['train']\n",
    "raw_datasets['validation'] = train_val_split['test']\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "input_column = column_names[0]\n",
    "output_column = column_names[1]\n",
    "\n",
    "# format dataset with dummy tokens\n",
    "special_tokens_dict[\"additional_special_tokens\"] = [dummy_token]\n",
    "\n",
    "def add_empty_token(x):\n",
    "    command_str = x[input_column]\n",
    "    command = command_str.split()\n",
    "    padded_command = []\n",
    "    index = 0\n",
    "    c = 0\n",
    "    while index < max_len:\n",
    "        expected_cs = command_structure[index]\n",
    "        if c < len(command) and command[c] in expected_cs:\n",
    "            padded_command.append(command[c])\n",
    "            c += 1\n",
    "        else:\n",
    "            padded_command.append(dummy_token)\n",
    "        index += 1\n",
    "    \n",
    "    x[input_column] = ' '.join(padded_command)\n",
    "    return x\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    raw_datasets[\"train\"] = raw_datasets[\"train\"].map(\n",
    "        add_empty_token,\n",
    "        batched=False,\n",
    "        num_proc=num_workers, \n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    raw_datasets[\"validation\"] = raw_datasets[\"validation\"].map(\n",
    "        add_empty_token,\n",
    "        batched=False,\n",
    "        num_proc=num_workers,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# left padding for batch generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "# Resize the embeddings only when necessary to avoid index errors\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Generation config\n",
    "generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n",
    "#gen_dict = generation_config.to_dict()\n",
    "#gen_dict[\"language\"] = model_lang\n",
    "# reload with new attributes\n",
    "#generation_config = GenerationConfig.from_dict(gen_dict)\n",
    "#max_gen_length = model.config.max_length\n",
    "#num_beams = args.num_beams if args.num_beams is not None else model.config.num_beams\n",
    "gen_kwargs = {\"max_new_tokens\": max_gen_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset\n",
    "def preprocess_function(examples):\n",
    "    # commands, actions\n",
    "    inputs = examples[input_column]\n",
    "    targets = examples[output_column]\n",
    "\n",
    "    # tokenize as single sequence separated by special token\n",
    "    model_inputs = tokenizer(\n",
    "        [i+tokenizer.sep_token for i in inputs],\n",
    "        padding='max_length', max_length=max_source_length\n",
    "    )\n",
    "    # labels same as inputs. labels shifted right in the model forward by default\n",
    "    model_inputs['labels'] = tokenizer(\n",
    "        [t+tokenizer.eos_token for t in targets],\n",
    "        padding='max_length', max_length=max_source_length\n",
    "    )['input_ids']\n",
    "    # set label padding to -100 \n",
    "    #model_inputs['labels'] = [\n",
    "        #[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_inputs['labels']\n",
    "    #]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    train_dataset = raw_datasets[\"train\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    eval_dataset = raw_datasets[\"validation\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator and loaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=train_steps * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare everything for accelerator\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_bar = tqdm(range(len(eval_dataloader)), position=0)\n",
    "accuracy = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    with torch.no_grad():\n",
    "        output_ids = accelerator.unwrap_model(model).generate(\n",
    "            **batch,\n",
    "            generation_config=generation_config,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "    # pad_acrss_processes to get equal length for each processs\n",
    "    output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "    label_ids = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "    # gather\n",
    "    output_ids = accelerator.gather(output_ids) \n",
    "    label_ids = accelerator.gather(label_ids)  \n",
    "    # decode\n",
    "    batch_output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    batch_input = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n",
    "    outputs = [batch_output[b].replace(batch_input[b], '') for b in range(len(batch_output))]\n",
    "    labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    # compute accuracy\n",
    "    acc = [o==l for o, l in zip(outputs, labels)]\n",
    "    accuracy += sum(acc)/len(acc)\n",
    "\n",
    "    eval_bar.update(1)\n",
    "\n",
    "print(accuracy/len(eval_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right padding for logits\n",
    "tokenizer.padding_side = \"right\"\n",
    "ignore_index = -100\n",
    "\n",
    "\n",
    "# re-tokenize left padded sequences need for batch generation to right padded sequences\n",
    "def re_tokenize(token_ids):\n",
    "    tokens = tokenizer.batch_decode(token_ids, skip_special_tokens=False)\n",
    "    tokens = [o.replace(tokenizer.pad_token, '') for o in tokens]\n",
    "    tokens = [o.replace(tokenizer.eos_token, '') for o in tokens]\n",
    "    tokenized_tokens = tokenizer(\n",
    "        tokens,\n",
    "        padding='max_length',\n",
    "        max_length=max_source_length,\n",
    "        return_tensors='pt',\n",
    "    ).to(model.device)\n",
    "    input_ids = tokenized_tokens['input_ids']\n",
    "    attention_mask = tokenized_tokens['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def prepare_input_for_rl_step(output_ids, gen_label_ids):\n",
    "\n",
    "    generated_ids, attention_mask = re_tokenize(output_ids)\n",
    "    gen_label_ids, _ = re_tokenize(gen_label_ids) \n",
    "    # context labels needed for ce loss for context\n",
    "    # get only context labels\n",
    "    all_tokens = tokenizer.batch_decode(generated_ids)\n",
    "    context_tokens = [t.split(tokenizer.sep_token)[0] for t in all_tokens]\n",
    "    tokenized_context = tokenizer(\n",
    "        [c+tokenizer.sep_token for c in context_tokens],\n",
    "        padding='max_length',\n",
    "        max_length=max_source_length,\n",
    "        return_tensors='pt',\n",
    "    ).to(model.device)\n",
    "    context_label_ids = tokenized_context['input_ids']\n",
    "    # set context label padding to -100 \n",
    "    context_label_ids = [\n",
    "        [\n",
    "            (l if l != tokenizer.pad_token_id else ignore_index) for l in label\n",
    "        ] for label in context_label_ids.tolist()\n",
    "    ]\n",
    "    context_label_ids = torch.tensor(context_label_ids).to(model.device)\n",
    "\n",
    "    return generated_ids, attention_mask, gen_label_ids, context_label_ids\n",
    "    \n",
    "    \n",
    "def reward_function(output_ids, gen_label_ids):\n",
    "    # decode output\n",
    "    output_tokens = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    output_tokens = [\n",
    "        o.replace(tokenizer.pad_token, '').split(tokenizer.sep_token)[1] for o in output_tokens\n",
    "    ]\n",
    "    # decode labels\n",
    "    label_tokens = tokenizer.batch_decode(gen_label_ids, skip_special_tokens=False)\n",
    "    label_tokens = [l.replace(tokenizer.pad_token, '') for l in label_tokens]\n",
    "    # compute reward(=accuracy)\n",
    "    reward = [o==l for o, l in zip(output_tokens, label_tokens)]\n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(model.device)\n",
    "    return reward\n",
    "\n",
    "\n",
    "def loss_function(logits, context_label_ids, gen_label_ids, attention_mask, reward):\n",
    "\n",
    "    # ce loss for context\n",
    "    # shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_context_labels = context_label_ids[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    context_loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), shift_context_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # construct context mask\n",
    "    context_mask = torch.ones_like(context_label_ids)\n",
    "    context_mask[context_label_ids != ignore_index] = 0\n",
    "    # apply mask on logits\n",
    "    print(logits)\n",
    "    logits[context_label_ids != ignore_index] = 0\n",
    "    print(logits)\n",
    "    raise\n",
    "\n",
    "    # construct end padding mask -> attention_mask\n",
    "\n",
    "\n",
    "    print(context_loss)\n",
    "    print(reward.shape)\n",
    "    print(logits.shape)\n",
    "    raise\n",
    "\n",
    "    # reinforce loss\n",
    "    # TODO:\n",
    "    \n",
    "    \n",
    "def reinforce_step(generated_ids, attention_mask, gen_label_ids, context_label_ids):\n",
    "    # calculate reward 'to go' (reinforce)\n",
    "    reward = reward_function(generated_ids, gen_label_ids)\n",
    "    # model forward\n",
    "    logits = model(input_ids=generated_ids, attention_mask=attention_mask).logits\n",
    "    # compute loss\n",
    "    loss_function(logits, context_label_ids, gen_label_ids, attention_mask, reward)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8fba95ad24486093f6abeb3317ce4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2dbc768cd44b8dbe39803e57cc4dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5630e+01, -1.4854e+01, -1.4864e+01,  ...,  3.2617e-01,\n",
      "          -5.9968e-02, -4.8850e-02],\n",
      "         [-8.3999e+01, -8.2473e+01, -8.6874e+01,  ...,  5.1035e-01,\n",
      "           4.3946e-03,  5.5931e-02],\n",
      "         [-1.1466e+01, -1.1571e+01, -1.2388e+01,  ...,  5.8126e-03,\n",
      "          -6.7820e-02,  8.5390e-02],\n",
      "         ...,\n",
      "         [ 2.7513e+01,  2.7691e+01,  2.9968e+01,  ..., -2.4414e-01,\n",
      "           2.1287e-01,  3.2635e-02],\n",
      "         [ 2.2583e+01,  2.2783e+01,  2.5139e+01,  ..., -1.4681e-01,\n",
      "          -7.4961e-02,  5.7500e-02],\n",
      "         [ 1.2865e+01,  1.3360e+01,  1.5330e+01,  ...,  3.3217e-01,\n",
      "           3.2817e-02, -5.1834e-02]],\n",
      "\n",
      "        [[-1.4374e+01, -1.3922e+01, -1.3552e+01,  ..., -2.0467e-01,\n",
      "           8.7158e-02, -2.7965e-02],\n",
      "         [-8.7493e+00, -8.3984e+00, -9.6252e+00,  ...,  1.8210e-01,\n",
      "           5.8279e-02, -1.8418e-02],\n",
      "         [-1.4499e+01, -1.4457e+01, -1.4552e+01,  ...,  2.0997e-01,\n",
      "           1.0126e-01,  5.3064e-02],\n",
      "         ...,\n",
      "         [ 2.8604e+01,  3.0093e+01,  2.9322e+01,  ..., -3.3581e-02,\n",
      "          -8.2852e-02,  1.1669e-01],\n",
      "         [ 2.2379e+01,  2.3894e+01,  2.4249e+01,  ..., -3.8978e-01,\n",
      "          -7.7454e-02,  7.8875e-02],\n",
      "         [ 2.6568e+01,  2.9173e+01,  2.9181e+01,  ..., -3.8368e-01,\n",
      "           1.2064e-01, -1.2211e-01]],\n",
      "\n",
      "        [[-1.5767e+01, -1.5281e+01, -1.5104e+01,  ...,  3.2617e-01,\n",
      "          -5.9968e-02, -4.8850e-02],\n",
      "         [-8.5785e+00, -8.2702e+00, -9.2439e+00,  ...,  5.1035e-01,\n",
      "           4.3946e-03,  5.5931e-02],\n",
      "         [-1.6504e+01, -1.6526e+01, -1.6483e+01,  ...,  5.8126e-03,\n",
      "          -6.7820e-02,  8.5390e-02],\n",
      "         ...,\n",
      "         [ 3.5324e+01,  3.8010e+01,  3.7112e+01,  ..., -2.4414e-01,\n",
      "           2.1287e-01,  3.2635e-02],\n",
      "         [ 4.0452e+01,  4.4003e+01,  4.5317e+01,  ..., -1.4681e-01,\n",
      "          -7.4961e-02,  5.7500e-02],\n",
      "         [ 2.3981e+01,  2.8541e+01,  3.0219e+01,  ...,  3.3217e-01,\n",
      "           3.2817e-02, -5.1834e-02]],\n",
      "\n",
      "        [[-1.2835e+01, -1.2554e+01, -1.2174e+01,  ..., -2.0467e-01,\n",
      "           8.7158e-02, -2.7965e-02],\n",
      "         [-9.6325e+00, -8.9196e+00, -1.0386e+01,  ...,  1.8210e-01,\n",
      "           5.8279e-02, -1.8418e-02],\n",
      "         [-6.6566e+00, -6.9999e+00, -6.9130e+00,  ...,  2.0997e-01,\n",
      "           1.0126e-01,  5.3064e-02],\n",
      "         ...,\n",
      "         [ 1.1409e+01,  1.2474e+01,  1.2130e+01,  ..., -3.3581e-02,\n",
      "          -8.2852e-02,  1.1669e-01],\n",
      "         [ 4.4405e+00,  6.4419e+00,  5.6677e+00,  ..., -3.8978e-01,\n",
      "          -7.7454e-02,  7.8875e-02],\n",
      "         [ 1.0681e+01,  1.1950e+01,  1.0869e+01,  ..., -3.8368e-01,\n",
      "           1.2064e-01, -1.2211e-01]]], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n",
      "tensor([[[-1.5630e+01, -1.4854e+01, -1.4864e+01,  ...,  3.2617e-01,\n",
      "          -5.9968e-02, -4.8850e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 2.7513e+01,  2.7691e+01,  2.9968e+01,  ..., -2.4414e-01,\n",
      "           2.1287e-01,  3.2635e-02],\n",
      "         [ 2.2583e+01,  2.2783e+01,  2.5139e+01,  ..., -1.4681e-01,\n",
      "          -7.4961e-02,  5.7500e-02],\n",
      "         [ 1.2865e+01,  1.3360e+01,  1.5330e+01,  ...,  3.3217e-01,\n",
      "           3.2817e-02, -5.1834e-02]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 2.8604e+01,  3.0093e+01,  2.9322e+01,  ..., -3.3581e-02,\n",
      "          -8.2852e-02,  1.1669e-01],\n",
      "         [ 2.2379e+01,  2.3894e+01,  2.4249e+01,  ..., -3.8978e-01,\n",
      "          -7.7454e-02,  7.8875e-02],\n",
      "         [ 2.6568e+01,  2.9173e+01,  2.9181e+01,  ..., -3.8368e-01,\n",
      "           1.2064e-01, -1.2211e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 3.5324e+01,  3.8010e+01,  3.7112e+01,  ..., -2.4414e-01,\n",
      "           2.1287e-01,  3.2635e-02],\n",
      "         [ 4.0452e+01,  4.4003e+01,  4.5317e+01,  ..., -1.4681e-01,\n",
      "          -7.4961e-02,  5.7500e-02],\n",
      "         [ 2.3981e+01,  2.8541e+01,  3.0219e+01,  ...,  3.3217e-01,\n",
      "           3.2817e-02, -5.1834e-02]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 1.1409e+01,  1.2474e+01,  1.2130e+01,  ..., -3.3581e-02,\n",
      "          -8.2852e-02,  1.1669e-01],\n",
      "         [ 4.4405e+00,  6.4419e+00,  5.6677e+00,  ..., -3.8978e-01,\n",
      "          -7.7454e-02,  7.8875e-02],\n",
      "         [ 1.0681e+01,  1.1950e+01,  1.0869e+01,  ..., -3.8368e-01,\n",
      "           1.2064e-01, -1.2211e-01]]], device='mps:0',\n",
      "       grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m generated_ids, attention_mask, gen_label_ids, context_label_ids \u001b[38;5;241m=\u001b[39m prepare_input_for_rl_step(output_ids, label_ids)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# reinforce\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mreinforce_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_label_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_label_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 101\u001b[0m, in \u001b[0;36mreinforce_step\u001b[0;34m(generated_ids, attention_mask, gen_label_ids, context_label_ids)\u001b[0m\n\u001b[1;32m     99\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mgenerated_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_label_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_label_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(logits, context_label_ids, gen_label_ids, attention_mask, reward)\u001b[0m\n\u001b[1;32m     79\u001b[0m logits[context_label_ids \u001b[38;5;241m!=\u001b[39m ignore_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# construct end padding mask -> attention_mask\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(context_loss)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "global_step = 0  # tracks total steps\n",
    "\n",
    "progress_bar = tqdm(range(global_step, train_steps), disable=not accelerator.is_main_process, position=0)\n",
    "# eval bar\n",
    "eval_bar = tqdm(range(len(eval_dataloader)), position=1)\n",
    "\n",
    "while True:\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        with torch.no_grad():\n",
    "            output_ids = accelerator.unwrap_model(model).generate(\n",
    "                **batch,\n",
    "                generation_config=generation_config,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "\n",
    "        # pad_acrss_processes to get equal length for each processs\n",
    "        output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "        label_ids = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        # gather\n",
    "        output_ids = accelerator.gather(output_ids) \n",
    "        label_ids = accelerator.gather(label_ids)  \n",
    "\n",
    "        # re-tokenize for rl step\n",
    "        # generated_ids -> context ids + generated action ids\n",
    "        # attention mask -> attention mask for generated_ids\n",
    "        # gen_label_ids -> generated action ids\n",
    "        # context_label_ids -> context ids, needed to compute ce loss for context\n",
    "        generated_ids, attention_mask, gen_label_ids, context_label_ids = prepare_input_for_rl_step(output_ids, label_ids)\n",
    "\n",
    "        # reinforce\n",
    "        reinforce_step(generated_ids, attention_mask, gen_label_ids, context_label_ids)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'turn <empty> right <empty> after turn around left thrice<sep>'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([15344,   220, 50259,   826,   220, 50259,   706,  1210,  1088,  1364, 5636,   501, 50258])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 1],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 1, 1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[0,0,1,1], [0,0,0,1], [0,1,1,1]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5., 5.],\n",
       "         [5., 5.],\n",
       "         [5., 5.],\n",
       "         [5., 5.]],\n",
       "\n",
       "        [[5., 5.],\n",
       "         [5., 5.],\n",
       "         [5., 5.],\n",
       "         [5., 5.]],\n",
       "\n",
       "        [[5., 5.],\n",
       "         [5., 5.],\n",
       "         [5., 5.],\n",
       "         [5., 5.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(3,4,2)*5\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a,a]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [5., 5.],\n",
       "         [5., 5.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [5., 5.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [5., 5.],\n",
       "         [5., 5.],\n",
       "         [5., 5.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(c,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
