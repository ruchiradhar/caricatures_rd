import random
import os
import json


def process_data(train_ratio):

    train = dict()
    val = dict()
    test = dict()
    categories = ["agents", "transitive_verbs", "things", "determiners",
                  "adverbs", "subject_adjectives", "object_adjectives"]
    for c in categories:
        with open(os.path.join("data", c + ".txt"), "r") as f:
            stuff = f.readlines()
            if c != "transitive_verbs":
                stuff = [_.strip() for _ in stuff]
            else:
                stuff = [_.strip().split() for _ in stuff]
        random.shuffle(stuff)
        if c != "determiners":
            train[c] = stuff[:int(len(stuff)*train_ratio)]
            val[c] = stuff[int(len(stuff)*train_ratio):int(len(stuff)*(train_ratio+(1-train_ratio)*0.5))]
            test[c] = stuff[int(len(stuff)*(train_ratio+(1-train_ratio)*0.5)):]
        else:
            train[c] = stuff
            val[c] = stuff
            test[c] = stuff
    return train, val, test


def restricted(restrictions, enc):
    # this function determines whether an encoding of an NLI input
    # is restricted according to restrictions
    if restrictions is None:
        return False
    if enc in restrictions:
        return False
    return True


def split_dict(filename, restrictions):  # simple_solutions
    
    # this function takes in a dictionary generated by build_simple_file or
    # build_boolean_file and divides the encoded NLI input keys by the label
    # they are mapped to
    with open(filename, 'r') as f:
        solutions = json.loads(f.read())
    e = dict()
    c = dict()
    p = dict()
    for i in solutions:
        if restricted(restrictions, i):
            continue
        if solutions[i] == "entails":
            e[i] = solutions[i]
        if solutions[i] == "contradicts":
            c[i] = solutions[i]
        if solutions[i] == "permits":
            p[i] = solutions[i]
    return e, c, p


def generate_balanced_data(
        simple_filename,    # simple_solutions
        boolean_filename,   # boolean_solutions
        simple_size,    # size defined in create_corpus
        boolean_size,   # 0
        data,   # data created in create_corpus
        simple_sampling="level 2",  # level 2
        boolean_sampling="level 1", # level 0
        keys_and_counts=None,
        restrictions=None
    ):

    e, c, p = split_dict(simple_filename, restrictions)
    ekeys = list(e.keys())
    ckeys = list(c.keys())
    pkeys = list(p.keys())
    if simple_sampling == "level 0" or simple_sampling == "level 2":
        ecounts, ccounts, pcounts = get_simple_encoding_counts(
            data, simple_sampling, ekeys, ckeys, pkeys)
    if simple_sampling == "level 1":
        ecounts = [1] * len(ekeys)
        ccounts = [1] * len(ckeys)
        pcounts = [1] * len(pkeys)
    label_size = int(simple_size/3)
    examples = []
    for i in range(label_size):
        encoding = json.loads(weighted_selection(ekeys, ecounts))
        premise, hypothesis = encoding_to_example(data, encoding)
        examples.append(
            (premise.emptystring, "entailment", hypothesis.emptystring))
    for i in range(label_size):
        encoding = json.loads(weighted_selection(ckeys, ccounts))
        premise, hypothesis = encoding_to_example(data, encoding)
        examples.append(
            (premise.emptystring, "contradiction", hypothesis.emptystring))
    for i in range(label_size):
        encoding = json.loads(weighted_selection(pkeys, pcounts))
        premise, hypothesis = encoding_to_example(data, encoding)
        examples.append(
            (premise.emptystring, "neutral", hypothesis.emptystring))
    bool_label_size = int(boolean_size/3)
    bool_e, bool_c, bool_p = split_dict(boolean_filename, None)
    bool_ekeys = list(bool_e.keys())
    bool_ckeys = list(bool_c.keys())
    bool_pkeys = list(bool_p.keys())
    if keys_and_counts == None:
        keys_and_counts = sevenclass_simple_encodings(
            data, 1, ekeys, ckeys, pkeys, ecounts, ccounts, pcounts)
    examples += generate_balanced_boolean_data(
        bool_ekeys, "entailment", keys_and_counts, boolean_sampling, bool_label_size, data)
    examples += generate_balanced_boolean_data(
        bool_ckeys, "contradiction", keys_and_counts, boolean_sampling, bool_label_size, data)
    examples += generate_balanced_boolean_data(
        bool_pkeys, "neutral", keys_and_counts, boolean_sampling, bool_label_size, data)
    random.shuffle(examples)
    return examples


def create_corpus():

    size = 500000
    filename = "1gendata"
    # data is a dict with `categories` as keys. Values are obtained from the respective files
    data, _, _ = process_data(1.0)  # train, val, test
    examples = generate_balanced_data("simple_solutions", "boolean_solutions", size, 0, data, simple_sampling="level 2", boolean_sampling="level 0")


if __name__ == '__main__':

    create_corpus()